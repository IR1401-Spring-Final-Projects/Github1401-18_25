{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_X3PsbzDtz"
      },
      "source": [
        "<div style=\"color:red;font-size:40px;\">\n",
        "<center>\n",
        "<font size=\"6\" >\n",
        "به نام خدا \n",
        "<br><br>\n",
        "<font size=\"5\" color=\"grey\" >\n",
        "<b>\n",
        "Github\n",
        "سایت\n",
        "README\n",
        "بازیابی بر روی فایل‌های \n",
        "\n",
        "</b>\n",
        "<br>\n",
        "<font size=\"4\" color=\"black\">\n",
        "دانشجویان:\n",
        "<br>\n",
        " (98101339) سایه جارالهی \n",
        "<br>\n",
        " (98171104) بردیا محمدی \n",
        "<br>\n",
        "امیررضا سلیمان‌بیگی (98109837)\n",
        "<br>\n",
        "استاد: دکتر احسان‌الدین عسگری\n",
        "</center>\n",
        "<br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY00D2DSCuzO"
      },
      "source": [
        "## Crawling\n",
        "\n",
        "First, the url of different repos are crawled. Consequently, for each repo, th README.md is crawled. Also, empty readmes are ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hYWf05w_C60h"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCAcM132C_v7"
      },
      "outputs": [],
      "source": [
        "import re \n",
        "\n",
        "def get_readme(url):\n",
        "    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
        "    html_code = soup.find(\"article\")\n",
        "    sents = []\n",
        "    for p in html_code.find_all(['td','p', 'li', re.compile('^h\\d+$')]):\n",
        "        text = p.get_text()\n",
        "        if text and text.replace('\\n', ''):\n",
        "            sents.append(text)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def scrape_readmes(url_list):\n",
        "    readmes = []\n",
        "    for idx, url in enumerate(url_list):\n",
        "        print(f\"{idx + 1} / {len(url_list)} | Accessing {url}\")\n",
        "        try:\n",
        "            readme = get_readme(url)\n",
        "            if len(readme):\n",
        "                readmes.append({\"url\": url, \"readme_html\": readme})\n",
        "        except:\n",
        "            print(f\"Couldn't access {url}\")\n",
        "        time.sleep(3)  # needed to not overload GitHub -> getting blocked\n",
        "    readmes_df = pd.DataFrame(readmes)\n",
        "    return readmes_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xcrPfCDrYTo"
      },
      "outputs": [],
      "source": [
        "topics = [\n",
        "    'game',\n",
        "    'hardware', \n",
        "    'computer+vision', \n",
        "    'artificial+intelligence', \n",
        "    'application', \n",
        "    'ios', \n",
        "    'multiplatform', \n",
        "    'deep+learning', \n",
        "    'cloud', \n",
        "    'verilog'\n",
        "]\n",
        "valid_topic_urls = [f'https://github.com/search?o=desc&p={i}&q={topic}&s=stars&type=Repositories' for i in range(1, 21) for topic in topics]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWxEBA4RrR2i"
      },
      "outputs": [],
      "source": [
        "def get_repo_urls(url):\n",
        "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
        "    repos = soup.find_all('a', class_='v-align-middle')\n",
        "    time.sleep(3) # needed to not overload GitHub -> getting blocked\n",
        "    return ['https://github.com'+repo.get('href') for repo in repos]\n",
        "\n",
        "def scrape_urls():\n",
        "    repo_urls = []\n",
        "    for i, url in enumerate(valid_topic_urls):\n",
        "        print('scraping url', i)\n",
        "        if url not in repo_urls:\n",
        "            repos = get_repo_urls(url)\n",
        "            repo_urls.extend(repos)\n",
        "    return repo_urls\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-HkTonE82zjQ"
      },
      "outputs": [],
      "source": [
        "repo_urls = scrape_urls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vwxfuu9DFJ7"
      },
      "outputs": [],
      "source": [
        "f = open('urls.json')\n",
        "repo_urls = json.load(f)\n",
        "readmes_df = scrape_readmes(repo_urls)\n",
        "readmes_df.to_json(\"readmes.json\", orient=\"records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23z-tA4zxuG"
      },
      "source": [
        "##Load data from files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Dq7xByPTz1JG"
      },
      "outputs": [],
      "source": [
        "readmes_df = pd.read_json('readmesjson.json', orient= 'records')\n",
        "tokens_df = pd.read_json('token.json', orient= 'records')\n",
        "paragraph_df = pd.read_json('string.json', orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFwZu2djRf8o"
      },
      "source": [
        "## Preprocess(Normalization, Tokenizarion, Removing Stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDP20wCKW9Os",
        "outputId": "63e4ab59-49e3-44f9-932e-44ffd1063a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CZ_7sQfYjR3T"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "class Preprocess:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer  = PorterStemmer()\n",
        "\n",
        "    def remove_emoji(self, string):\n",
        "        b'\\\\U0001f923'\n",
        "        emoj = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U0001f926-\\U0001f937\"\n",
        "            u\"\\U00010000-\\U0010ffff\"\n",
        "            u\"\\u2640-\\u2642\" \n",
        "            u\"\\u2600-\\u2B55\"\n",
        "            u\"\\u200d\"\n",
        "            u\"\\u23cf\"\n",
        "            u\"\\u23e9\"\n",
        "            u\"\\u231a\"\n",
        "            u\"\\ufe0f\"  # dingbats\n",
        "            u\"\\u3030\"\n",
        "                        \"]+\", re.UNICODE)\n",
        "        return emoj.sub(r'', string)\n",
        "\n",
        "    def normalize_tokens(self, tokens, remove_stopword, stopwords_domain=['https']):\n",
        "        \n",
        "        # Remove stopwords\n",
        "        if remove_stopword:\n",
        "            stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
        "            normalized_tokens = [word for word in tokens if (word.lower() not in stopwords_domain + stopwords)]\n",
        "        else:\n",
        "            normalized_tokens = tokens\n",
        "            \n",
        "        # remove links\n",
        "        normalized_tokens = [word for word in normalized_tokens if not re.match(r'^//.*(/?)$', word)]\n",
        "        \n",
        "        # Remove punctuations\n",
        "        normalized_tokens = [word for word in normalized_tokens if word not in string.punctuation]\n",
        "        \n",
        "        # Convert everything to lowercase\n",
        "        normalized_tokens = [word.lower() for word in normalized_tokens]\n",
        "\n",
        "        normalized_tokens = [word for word in normalized_tokens if len(word)>1]\n",
        "        \n",
        "        \n",
        "        # Convert numbers to \"Number\" token\n",
        "        normalized_tokens = [word if not re.match(r'(^| )\\d+(\\.\\d+)*($| )', word) else 'NUMBER' for word in normalized_tokens]\n",
        "            \n",
        "        return normalized_tokens\n",
        "\n",
        "    def isEnglish(self, s):\n",
        "        try:\n",
        "            s.encode(encoding='utf-8').decode('ascii')\n",
        "        except UnicodeDecodeError:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "        \n",
        "        \n",
        "    def non_english(self, text):\n",
        "        t_list = word_tokenize(text)\n",
        "        res = []\n",
        "        for i in t_list:\n",
        "            if self.isEnglish(i):\n",
        "                res.append(i)\n",
        "        return ' '.join(res)\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess(self, text, lemmatize=True, remove_stopword=True, token=True, re_sub_domain='\\'|`|-'):\n",
        "\n",
        "        # remove non enlish\n",
        "        text = self.non_english(text)\n",
        "\n",
        "        # replace sub_domain with space\n",
        "        text = re.sub(re_sub_domain, ' ', text)\n",
        "\n",
        "        # remove emoji\n",
        "        text = self.remove_emoji(text)\n",
        "\n",
        "        tokens = word_tokenize(text)\n",
        "        \n",
        "        # normalization\n",
        "        tokens = self.normalize_tokens(tokens, remove_stopword)\n",
        "        \n",
        "        # lemmatization\n",
        "        if lemmatize:\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "        \n",
        "        if token:\n",
        "            return tokens\n",
        "        else:\n",
        "            return ' '.join(tokens)\n",
        "\n",
        "\n",
        "    def get_preprocessed_token(self, file_path='readmes.json'):\n",
        "        data = json.loads(Path(file_path).read_text())\n",
        "        docs = []\n",
        "        for row in data:\n",
        "            text = row['readme_html']\n",
        "            preprocessed_tokens = []\n",
        "            for paragraph in text:\n",
        "                preprocessed_tokens += self.preprocess(paragraph)\n",
        "            docs.append({'url': row['url'], 'text': preprocessed_tokens})\n",
        "        Path('token.json').write_text(json.dumps(docs))\n",
        "\n",
        "\n",
        "    def get_preprocessed_str(self, file_path='readmes.json'):\n",
        "        data = json.loads(Path(file_path).read_text())\n",
        "        docs = []\n",
        "        for row in data:\n",
        "            text = row['readme_html']\n",
        "            preprocessed_paragraphs = []\n",
        "            for paragraph in text:\n",
        "                preprocessed_paragraphs.append(self.preprocess(paragraph, lemmatize=False, remove_stopword=False, token=False))\n",
        "            docs.append({'url': row['url'], 'text': preprocessed_paragraphs})\n",
        "        Path('string.json').write_text(json.dumps(docs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Preprocess().get_preprocessed_token()"
      ],
      "metadata": {
        "id": "c2a0uqPuDf45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_-g7HzDoxY"
      },
      "outputs": [],
      "source": [
        "Preprocess().get_preprocessed_str()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKeLJo3gD-fr"
      },
      "source": [
        "## Boolean Retrival\n",
        "In this section, boolean retrival is implemented. The format of input query should be: \n",
        "\n",
        "word1 AND word2 word3 OR word4 word5 NOT word6\n",
        "\n",
        "The matrix is implemented using inverted index. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FRKmlQ2mjIGL"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "import random\n",
        "from copy import deepcopy\n",
        "class BooleanRetrival:\n",
        "\n",
        "    def __init__(self, docs, k = 10):\n",
        "        self.inverted_index = {}\n",
        "        self.docs = docs\n",
        "        self.query_keywords = ['NOT', 'ORNOT', 'OR', 'AND']\n",
        "        self.k = k \n",
        "        self.create_inverted_index()\n",
        "    \n",
        "    def create_inverted_index(self):\n",
        "        for i, row in self.docs.iterrows():\n",
        "            tokens = row['text']\n",
        "            url = row['url']\n",
        "            for word in tokens:\n",
        "                if word in self.inverted_index.keys():\n",
        "                    self.inverted_index[word].add(url)\n",
        "                else:\n",
        "                    self.inverted_index.update({word:{url}})\n",
        "    \n",
        "    def dump_dict_json(self):\n",
        "        with open('inverted_index.json', 'w') as f:\n",
        "            f.write(json.dumps(self.inverted_index))\n",
        "\n",
        "    def add_and_to_query(self, query):\n",
        "        temp_query = deepcopy(query)\n",
        "        changed_count = 0 \n",
        "        for i in range(len(query)-1):\n",
        "            if query[i] not in self.query_keywords and query[i+1] not in self.query_keywords:\n",
        "                temp_query.insert(i+1 + changed_count,'AND')\n",
        "                changed_count +=1 \n",
        "        return temp_query\n",
        "    \n",
        "    def normalize_query(self, query):\n",
        "        result = []\n",
        "        for text in query:\n",
        "            if text not in self.query_keywords:\n",
        "                result.append(Preprocess().preprocess(text, token = False))\n",
        "            else:\n",
        "                result.append(text)\n",
        "        return result\n",
        "        \n",
        "    def tokenize_query(self, query):\n",
        "        query = query.replace('OR NOT', 'ORNOT')\n",
        "        query = query.split(' ')\n",
        "        query = self.add_and_to_query(query)\n",
        "        query = self.normalize_query(query)\n",
        "        return query\n",
        "    \n",
        "    def process_query(self, query, query_expansion = False):\n",
        "        if not query_expansion:\n",
        "            query = self.tokenize_query(query)\n",
        "        if not query[0] in self.inverted_index.keys():\n",
        "            result = set(self.docs.loc[:,\"url\"])\n",
        "        else:\n",
        "            result = set(self.inverted_index[query[0]])\n",
        "        i = 1\n",
        "        while i < len(query):\n",
        "            result = self.handle_operation(result,query[i+1], query[i])\n",
        "            i += 2\n",
        "        res = []\n",
        "        for i in result:\n",
        "            res.append({'score':1.0, 'url': i})\n",
        "        if query_expansion:\n",
        "            return result\n",
        "        return res\n",
        "    \n",
        "    def handle_operation(self, result:set, second_word:str, operation:str):\n",
        "        if not second_word in self.inverted_index.keys():\n",
        "            return result\n",
        "        second_list = self.inverted_index[second_word]\n",
        "        if operation == 'AND':\n",
        "            return result.intersection(second_list)\n",
        "        elif operation=='OR':\n",
        "            return result.union(second_list)\n",
        "        elif operation == 'ORNOT' :\n",
        "            all_urls = self.docs.loc[:,\"url\"]\n",
        "            return result.union((set(tokens_df.loc[:,'url'])) - self.inverted_index[second_word])\n",
        "        elif operation == 'NOT':\n",
        "            return result - second_list\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        ft_model = FastText.load('_fasttext.model')\n",
        "        query = self.tokenize_query(query)\n",
        "        all_queries = [query]\n",
        "        for i in range(5):\n",
        "            indexes = {}\n",
        "            for index, part in enumerate(query):\n",
        "                if part in ['ORNOT', 'NOT', 'OR', 'AND']:\n",
        "                    continue\n",
        "                similars = [word for word, percentage in ft_model.most_similar([part]) if percentage>0.85]\n",
        "                indexes.update({index: similars})\n",
        "            temp, query = self.generate_queries(indexes, query)\n",
        "            all_queries.extend(temp)\n",
        "            all_queries.append(query)\n",
        "        res = set()\n",
        "        for query in all_queries:\n",
        "            res.update(self.process_query(query, True))\n",
        "        return res\n",
        "            \n",
        "\n",
        "\n",
        "    def generate_queries(self, indexes, query):\n",
        "        final_query = deepcopy(query)\n",
        "        new_queries = []\n",
        "        for index, similars in indexes.items():\n",
        "            if similars:\n",
        "                final_query[index] = random.choice(similars)\n",
        "                q = deepcopy(query)\n",
        "                q[index] = random.choice(similars)\n",
        "                new_queries.append(q)\n",
        "        return new_queries, final_query\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "t_nWoI7s_JU0"
      },
      "outputs": [],
      "source": [
        "from preprocess import tokens_df\n",
        "model = BooleanRetrival(tokens_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Gefyl6qfXaU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04074cad-c589-443a-f3c6-0838d1a2e044"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 1.0, 'url': 'https://github.com/microsoft/DirectML'},\n",
              " {'score': 1.0, 'url': 'https://github.com/BradLarson/GPUImage'},\n",
              " {'score': 1.0, 'url': 'https://github.com/chipsalliance/Surelog'},\n",
              " {'score': 1.0, 'url': 'https://github.com/NervanaSystems/neon'},\n",
              " {'score': 1.0, 'url': 'https://github.com/oshi/oshi'},\n",
              " {'score': 1.0,\n",
              "  'url': 'https://github.com/ahkarami/Deep-Learning-in-Production'},\n",
              " {'score': 1.0, 'url': 'https://github.com/cocotb/cocotb'},\n",
              " {'score': 1.0, 'url': 'https://github.com/PaddlePaddle/Paddle'},\n",
              " {'score': 1.0,\n",
              "  'url': 'https://github.com/robmarkcole/satellite-image-deep-learning'},\n",
              " {'score': 1.0, 'url': 'https://github.com/pikvm/pikvm'},\n",
              " {'score': 1.0, 'url': 'https://github.com/matteocrippa/awesome-swift'},\n",
              " {'score': 1.0, 'url': 'https://github.com/nolimits4web/swiper'},\n",
              " {'score': 1.0, 'url': 'https://github.com/ellisonleao/magictools'},\n",
              " {'score': 1.0,\n",
              "  'url': 'https://github.com/awesome-selfhosted/awesome-selfhosted'},\n",
              " {'score': 1.0, 'url': 'https://github.com/ory/hydra'},\n",
              " {'score': 1.0, 'url': 'https://github.com/Tencent/Hardcoder'},\n",
              " {'score': 1.0, 'url': 'https://github.com/path/FastImageCache'},\n",
              " {'score': 1.0, 'url': 'https://github.com/vsouza/awesome-ios'},\n",
              " {'score': 1.0, 'url': 'https://github.com/apple/swift-corelibs-libdispatch'},\n",
              " {'score': 1.0, 'url': 'https://github.com/apache/incubator-mxnet'},\n",
              " {'score': 1.0, 'url': 'https://github.com/streamlinevideo/streamline'},\n",
              " {'score': 1.0, 'url': 'https://github.com/0015/ThatProject'},\n",
              " {'score': 1.0, 'url': 'https://github.com/pi-hole/pi-hole'},\n",
              " {'score': 1.0, 'url': 'https://github.com/gbdev/awesome-gbdev'},\n",
              " {'score': 1.0, 'url': 'https://github.com/JoseDeFreitas/awesome-youtubers'},\n",
              " {'score': 1.0, 'url': 'https://github.com/alibaba/MNN'},\n",
              " {'score': 1.0, 'url': 'https://github.com/donnemartin/awesome-aws'},\n",
              " {'score': 1.0, 'url': 'https://github.com/Nic30/hwt'},\n",
              " {'score': 1.0, 'url': 'https://github.com/aymericdamien/TopDeepLearning'},\n",
              " {'score': 1.0,\n",
              "  'url': 'https://github.com/mahmoud/awesome-python-applications'},\n",
              " {'score': 1.0,\n",
              "  'url': 'https://github.com/luong-komorebi/Awesome-Linux-Software'},\n",
              " {'score': 1.0, 'url': 'https://github.com/NVIDIA/nvvl'},\n",
              " {'score': 1.0, 'url': 'https://github.com/chipsalliance/chisel3'},\n",
              " {'score': 1.0, 'url': 'https://github.com/hybridgroup/gobot'},\n",
              " {'score': 1.0, 'url': 'https://github.com/IntelRealSense/librealsense'},\n",
              " {'score': 1.0, 'url': 'https://github.com/WICG/webusb'},\n",
              " {'score': 1.0,\n",
              "  'url': 'https://github.com/ben-marshall/awesome-open-hardware-verification'},\n",
              " {'score': 1.0, 'url': 'https://github.com/PySimpleGUI/PySimpleGUI'},\n",
              " {'score': 1.0, 'url': 'https://github.com/onceupon/Bash-Oneliner'},\n",
              " {'score': 1.0, 'url': 'https://github.com/Gamua/Starling-Framework'},\n",
              " {'score': 1.0, 'url': 'https://github.com/rakyll/go-hardware'},\n",
              " {'score': 1.0,\n",
              "  'url': 'https://github.com/jaredthecoder/awesome-vehicle-security'},\n",
              " {'score': 1.0, 'url': 'https://github.com/microsoft/CNTK'}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "result = model.process_query('Hardware cores OR Arduino AND API')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow9bcsMvY0dM",
        "outputId": "107dd629-418e-439e-91cf-ed7ca3af9779"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.expand_query('Hardware cores OR Arduino AND API')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33t5gJIMXTEx",
        "outputId": "e64db089-2a83-4390-d81f-8db163eb5903"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XH-iAg3MyLk"
      },
      "source": [
        "## Shunt algorithm: \n",
        "This can be used if our queries have paranthesis or they are inside each other. As was mentioned in quera, there is no need for the boolean retrival module to handle queries with paranthesis. However, we implemented that feature. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6vOsS5MMlYL"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "opinfo = namedtuple('Operator', 'precedence associativity')\n",
        "operator_info = {\n",
        "    \"AND\": opinfo(0, \"L\"),\n",
        "    \"OR\": opinfo(0, \"L\"),\n",
        "    \"NOT\": opinfo(0, \"L\"),\n",
        "    \"ORNOT\": opinfo(0,\"L\")\n",
        "}\n",
        "\n",
        "def shunt(tokens):\n",
        "    tokens += ['end']\n",
        "    operators = []\n",
        "    output = []\n",
        "    while len(tokens) != 1:\n",
        "        current_token = tokens.pop(0)\n",
        "\n",
        "        if current_token not in operator_info.keys() and current_token not in ['(', ')']:\n",
        "            output.append(current_token)\n",
        "        elif current_token in operator_info.keys():\n",
        "            while len(operators):\n",
        "                satisfied = False\n",
        "                if operators[-1] not in [\"(\", \")\"]:\n",
        "                    satisfied = True\n",
        "                satisfied = satisfied and operators[-1] != \"(\"\n",
        "                if not satisfied:\n",
        "                    break\n",
        "                output.append(operators.pop())\n",
        "            operators.append(current_token)\n",
        "        elif current_token == \"(\":\n",
        "            operators.append(current_token)\n",
        "\n",
        "        elif current_token == \")\":\n",
        "            while len(operators) and operators[-1] != '(':\n",
        "                output.append(operators.pop())\n",
        "            if len(operators) != 0 and operators[-1] == \"(\":\n",
        "                operators.pop()\n",
        "    output.extend(operators[::-1])\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf-idf"
      ],
      "metadata": {
        "id": "r7hA1FH4c3V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class Tfidf:\n",
        "    \n",
        "    def __init__(self, file_name='token.json', k = 10):\n",
        "        data = json.loads(Path(file_name).read_text())\n",
        "        corpus = []\n",
        "        self.urls = []\n",
        "        for row in data:\n",
        "            corpus.append(' '.join(row['text']))\n",
        "            self.urls.append(row['url'])\n",
        "\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            input='content', \n",
        "            lowercase=False, \n",
        "            analyzer='word', \n",
        "            ngram_range=(1, 1), \n",
        "            max_df=0.90, \n",
        "            min_df=1, \n",
        "            max_features=None, \n",
        "            vocabulary=None, \n",
        "            binary=False, \n",
        "            norm='l2', \n",
        "            use_idf=True, \n",
        "            smooth_idf=True, \n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        self.k = k\n",
        "        self.X = self.vectorizer.fit_transform(corpus)\n",
        "        self.words = self.vectorizer.get_feature_names_out().tolist()\n",
        "        \n",
        "    def process_query(self, query, url_num=10, expand = False):\n",
        "        query = ' '.join(Preprocess().preprocess(query))\n",
        "        query_embedding = self.vectorizer.transform([query]).T\n",
        "        if expand:\n",
        "            return self.expand_query(query_embedding.T)     \n",
        "        score = self.X.dot(query_embedding).toarray()[:, 0]\n",
        "        max_scores_docs = list(np.argsort(score))\n",
        "        \n",
        "        related_urls = []\n",
        "        scores = []\n",
        "        for doc in reversed(max_scores_docs):\n",
        "            if self.urls[doc] not in related_urls:\n",
        "                related_urls.append(self.urls[doc])\n",
        "                scores.append(score[doc])\n",
        "            if len(related_urls) == url_num:\n",
        "                break\n",
        "            \n",
        "        return related_urls, scores\n",
        "        \n",
        "\n",
        "    def print_results(self, query, expand = False):\n",
        "        for url, score in zip(*self.process_query(query, expand =  expand)):\n",
        "            print(f'url: {url}, score: {score:.5f}')\n",
        "        \n",
        "    def expand_query(self,query):\n",
        "        _, url, cos_sim = Rocchio().expand_query(self.X.toarray(), query.toarray()[0], self.urls)\n",
        "        return url[0][0:self.k], cos_sim[0][0:self.k]"
      ],
      "metadata": {
        "id": "yW9kev-Yc-Ri"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Tfidf()\n"
      ],
      "metadata": {
        "id": "3377Aje2dSDq"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_results(query='face detection')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFndYNQuYD4C",
        "outputId": "af9931e6-984f-4bfb-b163-fb8a4a8078bc"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url: https://github.com/cvzone/cvzone, score: 0.28570\n",
            "url: https://github.com/arunponnusamy/cvlib, score: 0.15126\n",
            "url: https://github.com/php-opencv/php-opencv-examples, score: 0.14787\n",
            "url: https://github.com/chandrikadeb7/Face-Mask-Detection, score: 0.14187\n",
            "url: https://github.com/eduardolundgren/tracking.js, score: 0.14162\n",
            "url: https://github.com/mpatacchiola/deepgaze, score: 0.13612\n",
            "url: https://github.com/bnosac/image, score: 0.13437\n",
            "url: https://github.com/symisc/sod, score: 0.13375\n",
            "url: https://github.com/extreme-assistant/survey-computer-vision-2020, score: 0.12323\n",
            "url: https://github.com/amusi/awesome-ai-awesomeness, score: 0.11560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_results(query = 'face detection', expand = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBW2q5OCKKlg",
        "outputId": "36d1c0db-1276-41a3-d8b8-de779b8e3ab2"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1475, 59504) (1, 59504) <class 'list'>\n",
            "<class 'numpy.ndarray'>\n",
            "url: https://github.com/cvzone/cvzone, score: 0.46401\n",
            "url: https://github.com/arunponnusamy/cvlib, score: 0.35401\n",
            "url: https://github.com/mpatacchiola/deepgaze, score: 0.35140\n",
            "url: https://github.com/php-opencv/php-opencv-examples, score: 0.33587\n",
            "url: https://github.com/chandrikadeb7/Face-Mask-Detection, score: 0.32436\n",
            "url: https://github.com/bnosac/image, score: 0.32299\n",
            "url: https://github.com/eduardolundgren/tracking.js, score: 0.32273\n",
            "url: https://github.com/symisc/sod, score: 0.20424\n",
            "url: https://github.com/rchavezj/OpenCV_Projects, score: 0.19810\n",
            "url: https://github.com/amzn/computer-vision-basics-in-microsoft-excel, score: 0.18396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC3ZFJuxCC1L"
      },
      "source": [
        "## FastText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install engines\n"
      ],
      "metadata": {
        "id": "M6HnhD97hy0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "QdKNuz-mCCWa"
      },
      "outputs": [],
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "\n",
        "class FastTextModel:\n",
        "\n",
        "    def __init__(self, data, train=False, k=-1):\n",
        "        self.data = data\n",
        "        if train:\n",
        "            self.ft_model = self.train()\n",
        "        else:\n",
        "            self.ft_model = FastText.load('_fasttext.model')\n",
        "        self.data_embedding = self.get_data_embedding_avg()\n",
        "        self.k = k\n",
        "\n",
        "    def get_words(self, doc):\n",
        "        words = []\n",
        "        for sent in doc:\n",
        "            words.extend(sent.split(' '))\n",
        "        return words\n",
        "\n",
        "    def get_tok_text(self):\n",
        "        tok_text = []\n",
        "        for doc in self.data['readme_html']:\n",
        "            tok_text.append(self.get_words(doc))\n",
        "        return tok_text\n",
        "\n",
        "    def train(self):\n",
        "        ft_model = FastText(sg=1, size=100, window=10, min_count=2, negative=15, min_n=2, max_n=5)\n",
        "        tok_text = list(self.data.loc[:, 'text'])\n",
        "        ft_model.build_vocab(tok_text)\n",
        "        ft_model.train(\n",
        "            tok_text,\n",
        "            epochs=6,\n",
        "            total_examples=ft_model.corpus_count,\n",
        "            total_words=ft_model.corpus_total_words)\n",
        "        ft_model.save('_fasttext.model')  # save\n",
        "        return ft_model\n",
        "\n",
        "    def get_data_embedding_avg(self) -> dict:\n",
        "        '''\n",
        "\n",
        "        :return: a dict: the keys are the urls of data and the value is the embedding average\n",
        "        '''\n",
        "        docs_avg = dict()\n",
        "        for index, row in self.data.iterrows():\n",
        "            words = row['text']\n",
        "            url = row['url']\n",
        "            docs_avg[url] = np.mean([self.ft_model.wv[word] for word in words], axis=0)\n",
        "        return docs_avg\n",
        "\n",
        "    def get_query_embedding(self, query_tokens):\n",
        "        return np.mean([self.ft_model.wv[word] for word in query_tokens], axis=0)\n",
        "\n",
        "    def tokenize_query(self, query: str):\n",
        "        query = Preprocess().preprocess(query, token=True)\n",
        "        return query\n",
        "\n",
        "    def search_query(self, query, expand = False):\n",
        "        tokens = self.tokenize_query(query)\n",
        "        q_embedding = self.get_query_embedding(tokens)\n",
        "        if expand:\n",
        "            return self.expand_query(q_embedding)\n",
        "        cosine_sim_values = {}\n",
        "        for url, embedding_value in self.data_embedding.items():\n",
        "            cosine_sim_values.update({url: self.cosine_sim(embedding_value, q_embedding)})\n",
        "        if not expand:\n",
        "            return self.calculate_best_k(cosine_sim_values)\n",
        "\n",
        "\n",
        "    def calculate_best_k(self, cosine_sim_values:dict):\n",
        "        res = sorted(cosine_sim_values.items(), key=lambda x: x[1], reverse = True) [0: self.k]\n",
        "        return res\n",
        "\n",
        "    def cosine_sim(self, x, y):\n",
        "        return 1- scipy.spatial.distance.cosine(x, y)\n",
        "\n",
        "    def expand_query(self,q_embedding ):\n",
        "        _, url, cos_sim = Rocchio().expand_query([x for u, x in self.data_embedding.items()],q_embedding, [u for u, x in self.data_embedding.items()])\n",
        "        res = dict()\n",
        "        for i in range(self.k):\n",
        "            res.update({url[0][i]:cos_sim[0][i]})\n",
        "        return res.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raItU7AqGa0L"
      },
      "source": [
        "## Initialize FastText model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "cNtirY4jGQpb"
      },
      "outputs": [],
      "source": [
        "# by setting train value to True, you have to wait about 8 minutes for the training process\n",
        "ft_model = FastTextModel(tokens_df, train=False, k=10)\n",
        "a = ft_model.search_query('atari game')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvI3Z48SjhgI",
        "outputId": "8d3b53a5-3af6-4a13-e37c-25b95c69bdda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://github.com/itsFrank/MinecraftHDL', 0.8557753562927246),\n",
              " ('https://github.com/dawsonjon/Chips-2.0', 0.8528292775154114),\n",
              " ('https://github.com/yadox666/The-Hackers-Hardware-Toolkit',\n",
              "  0.8504918813705444),\n",
              " ('https://github.com/kitspace/awesome-electronics', 0.8503836989402771),\n",
              " ('https://github.com/fossasia/neurolab-hardware', 0.8484646081924438),\n",
              " ('https://github.com/aappleby/MetroBoy', 0.8444978594779968),\n",
              " ('https://github.com/fossasia/neurolab-firmware', 0.8440712690353394),\n",
              " ('https://github.com/pervognsen/bitwise', 0.8423163890838623),\n",
              " ('https://github.com/OpnTec/open-spectrometer-hardware', 0.8421347737312317),\n",
              " ('https://github.com/stevehoover/warp-v', 0.8387736678123474)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "a = ft_model.search_query('internet of things circuit')\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = ft_model.search_query('internet of things circuit', expand = True)\n",
        "list(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2IYc2VhgoUY",
        "outputId": "c6e09d5c-23df-4f7a-8250-533f7cb80df9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://github.com/OpnTec/open-spectrometer-hardware', 0.9373492),\n",
              " ('https://github.com/yadox666/The-Hackers-Hardware-Toolkit', 0.9358752),\n",
              " ('https://github.com/kitspace/awesome-electronics', 0.93562627),\n",
              " ('https://github.com/dawsonjon/Chips-2.0', 0.93493414),\n",
              " ('https://github.com/fossasia/neurolab-hardware', 0.93459404),\n",
              " ('https://github.com/itsFrank/MinecraftHDL', 0.93376243),\n",
              " ('https://github.com/pervognsen/bitwise', 0.933619),\n",
              " ('https://github.com/fossasia/neurolab-firmware', 0.93012476),\n",
              " ('https://github.com/m-labs/migen', 0.9258059),\n",
              " ('https://github.com/stevehoover/warp-v', 0.92271626)]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y93dsx9ddU4f"
      },
      "source": [
        "## Initialize BERT model \n",
        "\n",
        "We will use the distilbert-base-nli-stsb-mean-tokens model.\n",
        "\n",
        "Faiss is a library for efficient similarity search and clustering of dense vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "dBxQWTGqwHSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Hxc5MLhedPzZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scipy \n",
        "from operator import itemgetter\n",
        "\n",
        "class TransformerModel:\n",
        "\n",
        "    def __init__(self, data, k = 10, load_data = True):\n",
        "        self.k = 10\n",
        "        self.model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "        if torch.cuda.is_available():\n",
        "            model = self.model.to(torch.device(\"cuda\"))\n",
        "        self.data = data\n",
        "        self.list_to_string()\n",
        "        self.set_id()\n",
        "        if load_data:\n",
        "            self.embedding = np.load('bert_embedding.npy')\n",
        "        else:\n",
        "            self.embedding = self.get_data_embedding()\n",
        "        self.index = self.set_indexes()\n",
        "\n",
        "    def list_to_string(self):\n",
        "      #Change list of doc string to string\n",
        "        for i in range(0, len(self.data['readme_html'])):\n",
        "          self.data['readme_html'][i] = ' '.join(map(str, self.data['readme_html'][i]))\n",
        "\n",
        "    def get_data_embedding(self):\n",
        "        return self.model.encode(self.data['readme_html'].tolist(), show_progress_bar=True)\n",
        "\n",
        "    def vector_search(self, query, model, index, num_results=10):\n",
        "        vector = model.encode(list(query))\n",
        "        D, I = index.search(np.array(vector).astype(\"float32\"), k=num_results)\n",
        "        return D, I\n",
        "\n",
        "    def set_id(self):\n",
        "        self.data['id'] = range(0, len(self.data))\n",
        "    \n",
        "    def set_indexes(self):\n",
        "        embeddings = np.array([embedding for embedding in self.embedding]).astype(\"float32\")\n",
        "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "        index = faiss.IndexIDMap(index)\n",
        "        index.add_with_ids(embeddings, self.data.id.values)\n",
        "        return index\n",
        "\n",
        "    def get_recommendation(self, user_query):\n",
        "        D, I = self.vector_search([user_query], self.model, self.index, num_results=10)\n",
        "        print(f'L2 distance: {D.flatten().tolist()}\\n\\nReadme IDs: {I.flatten().tolist()}')\n",
        "        return [self.data['url'][i] for i in I.flatten().tolist()]\n",
        "\n",
        "    def get_recommendation2(self, query, expand = False):\n",
        "        queries = [query]\n",
        "        query_embeddings = self.model.encode(queries)\n",
        "        if expand:\n",
        "            return self.expand_query(query_embeddings[0])\n",
        "        number_top_matches = 10\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\n",
        "            distances = scipy.spatial.distance.cdist(query_embeddings, self.embedding, \"cosine\")[0]\n",
        "            results = zip(range(len(distances)), distances)\n",
        "            results = sorted(results, key=lambda x: x[1])\n",
        "            temp = []\n",
        "            for idx, distance in results[0:number_top_matches]:\n",
        "              temp.append(str(1-distance) + ' ' + readmes_df['url'][idx])\n",
        "            return temp\n",
        "\n",
        "    def expand_query(self,q_embedding ):\n",
        "        _, url, cos_sim = Rocchio().expand_query(self.embedding,q_embedding, np.array(readmes_df['url']))\n",
        "        print('212')\n",
        "        temp = []\n",
        "        for i in range(self.k):\n",
        "            temp.append(str(cos_sim[0][i]) + ' '+ url[0][i])\n",
        "        return temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "lMvZJQLlklyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de2e4a9-5d09-4d7f-a3cb-601a8d5653dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "t_model = TransformerModel(readmes_df)\n",
        "a = t_model.get_recommendation2('password manager')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ERWoq2Bzazd",
        "outputId": "fbb33762-f4cd-41cc-8ba3-b9d7d907e7af"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.6634643535739695 https://github.com/marcwebbie/passpie',\n",
              " '0.6069893094109988 https://github.com/keepassxreboot/keepassxc',\n",
              " '0.457747353179063 https://github.com/nndl/nndl.github.io',\n",
              " '0.4214187736338315 https://github.com/nondanee/UnblockNeteaseMusic',\n",
              " '0.41270995062754745 https://github.com/blinker-iot/blinker-js',\n",
              " '0.41015671358868366 https://github.com/jpillora/cloud-torrent',\n",
              " '0.4077809766286429 https://github.com/relativty/Relativty',\n",
              " '0.4050003055261049 https://github.com/pikvm/pikvm',\n",
              " '0.4022530244289647 https://github.com/ndeadly/MissionControl',\n",
              " '0.3958997887211033 https://github.com/FiloSottile/mkcert']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = t_model.get_recommendation2('password manager', True)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lfkF9Q9prXe",
        "outputId": "edbcb9c8-1e64-4811-c493-e9921208190e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.8459593 https://github.com/marcwebbie/passpie',\n",
              " '0.7843307 https://github.com/keepassxreboot/keepassxc',\n",
              " '0.678036 https://github.com/nondanee/UnblockNeteaseMusic',\n",
              " '0.6655154 https://github.com/jpillora/cloud-torrent',\n",
              " '0.6591674 https://github.com/Vove7/yyets_flutter',\n",
              " '0.63441026 https://github.com/blinker-iot/blinker-js',\n",
              " '0.62113225 https://github.com/ish-app/ish',\n",
              " '0.61276305 https://github.com/ArcadeRenegade/SidebarDiagnostics',\n",
              " '0.6116522 https://github.com/relativty/Relativty',\n",
              " '0.60775113 https://github.com/canonical/multipass']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report"
      ],
      "metadata": {
        "id": "v3ujQ5pOBozu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaDaJa8iKr3a"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font size=5>\n",
        "<b>\n",
        "گزارش انجام پروژه:\n",
        "</b>\n",
        "<font size =3>\n",
        "<br>\n",
        "در این تمرین، فایل‌های README گیت‌هاب با استفاده از crawling جمع آوری شده و پس از انجام پیش پردازش بر روی آن‎‌ها، چهار مدل مختلف برای بازیابی اطلاعات و جستجوی مطالب قرار گرفته است. \n",
        "<br>\n",
        "<b>Crawling:</b><br>\n",
        "در گام اول Crawling داده انجام شده است. با توجه به اینکه در گیت‌هاب ریپازیتوری های مختلفی با موضوعات بسیار گوناگون وجود دارد، با جمع آوری داده در اندازه محدود‌ (حدود ۱۶۰۰ ریپازیتوری)‌ نمیتوان به طور مناسب جستجو را انجام داد. به این جهت، کرال کردن داده ها بر روی ۱۰ دسته کلی از ریپازیتوری‌ها (بازی، سخت افزار، بینایی ماشین، هوش مصنوعی، اپلیکیشن، ios، مولتی پلتفرم، دیپ لرنینگ، فضای ابری، وریلاگ) انجام شده است. از هریک از این ریپازیتوری ها 20 صفحه ( که هر یک شامل 10 ریپازیتوری است) ذخیره شده و README آنها مورد بررسی قرار گرفته است. \n",
        "<br>\n",
        "<b>PreProcessing:</b>\n",
        "<br>\n",
        "چالش بعدی در این زمینه، Preprocessing بر روی داده ها برای رسیدن به داده مطلوب بوده است. در این بخش، در صورتی که داده ها به زبانی به جز انگلیسی باشند (موردی که مشاهده شد آن است که داده هایی به زبان چینی و هندی بسیار رایج است) آن داده ها را حذف میکند. همچنین از lemmatizer استفاده میشود تا کلمات همخانواده به یک کلمه یکسان بروند. در نهایت نیز از تمام اعداد با عبارت NUMBER جانشین شده، از Tokenizerو Normalizer و Stopword Removal استفاده شده است تا خروجی مناسب تولید شود. در برخی از الگوریتم های بازیابی از توکن ها و در برخی از متن کامل نورمالایزر شده استفاده میشود. همچنین در بسیاری از repository ها لینک‌هایی وجود دارد که آنها را نیز حذف می‌کنیم. در صورتی طول کلمه کمتر از 2 باشد آن را حذف می‌کنیم. همچنین emoji های موجود را نیز حذف می‌کنیم.\n",
        "<br>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydLEkNbSY1z5"
      },
      "source": [
        "<div dir= \"rtl\">\n",
        "<font size =3>\n",
        "<b>Boolean Retrival:</b>\n",
        "<br>\n",
        "در گام اول جستجو Boolean انجام می‌شود. این جستجو تنها به حضور کلمات در متن‌ها توجه دارد. در نتیجه هرچه تعداد کلمات کوئری‌ها بیشتر باشد و این کلمات به هم نامرتبط باشد،‌ تعداد خروجی‌ها نیز کمتر می‌شود. برای استفاده از این الگوریتم، ابتدا inverted matrix از روی داده‌ها به دست آورده میشود و در بخش بعدی، پاسخ دادن به کوئری‌ها انجام می‌شود. فرمت کوئری‌ها به صورتی است که تعدادی کلمه است که با AND, OR, NOT, OR NOT جدا شده اند. درصورتی‌که بین دو کلمه هیچ عبارت متصل کننده‌ای نباشد نیز AND در نظر گرفته میشود. \n",
        "مقدار mrr عدد 0.61 است\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.process_query('atari game NOT deep')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPe4D3Rbx95c",
        "outputId": "dfd80f3e-5d70-4228-b2f7-f4dc510e92e3"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['https://github.com/keras-rl/keras-rl',\n",
              "  'https://github.com/OpenEmu/OpenEmu',\n",
              "  'https://github.com/yenchenlin/DeepLearningFlappyBird',\n",
              "  'https://github.com/amusi/Deep-Learning-Interview-Book',\n",
              "  'https://github.com/ardamavi/Game-Bot',\n",
              "  'https://github.com/watabou/pixel-dungeon',\n",
              "  'https://github.com/KestrelComputer/kestrel',\n",
              "  'https://github.com/simoninithomas/Deep_reinforcement_learning_Course',\n",
              "  'https://github.com/charcole/Z3',\n",
              "  'https://github.com/andri27-ts/Reinforcement-Learning'],\n",
              " [0.2043208221190701,\n",
              "  0.17373314306640214,\n",
              "  0.14216671035472217,\n",
              "  0.12498932585085298,\n",
              "  0.10777785843395163,\n",
              "  0.10263801713559492,\n",
              "  0.0996560499631102,\n",
              "  0.09510212303684056,\n",
              "  0.0913781407004026,\n",
              "  0.0900197024776015])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.process_query('cloud service OR IoT AND docker')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TybdeIf8_mtT",
        "outputId": "a21dd36b-fd9d-479c-b3ab-b5d1970fb251"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['https://github.com/thingsboard/thingsboard',\n",
              "  'https://github.com/wxiaoqi/Spring-Cloud-Platform',\n",
              "  'https://github.com/macrozheng/springcloud-learning',\n",
              "  'https://github.com/dyc87112/SpringCloud-Learning',\n",
              "  'https://github.com/localstack/localstack',\n",
              "  'https://github.com/docker/compose',\n",
              "  'https://github.com/netdata/netdata',\n",
              "  'https://github.com/paascloud/paascloud-master',\n",
              "  'https://github.com/blinker-iot/blinker-library',\n",
              "  'https://github.com/ityouknow/spring-cloud-examples'],\n",
              " [0.22630859780378812,\n",
              "  0.189461093555495,\n",
              "  0.1836757297692159,\n",
              "  0.16341134293341783,\n",
              "  0.15013899194256636,\n",
              "  0.1421791969205611,\n",
              "  0.12978398410466505,\n",
              "  0.12674046466582461,\n",
              "  0.12633112249324804,\n",
              "  0.1190655953187189])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.process_query('hybrid transactional processing OR recommender OR system')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn9_yt2WJBoy",
        "outputId": "96c1ca8b-2ea3-44dc-ebf0-eb275da5842a"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['https://github.com/pingcap/tidb',\n",
              "  'https://github.com/d2l-ai/d2l-en',\n",
              "  'https://github.com/shenweichen/DeepCTR',\n",
              "  'https://github.com/cryptomator/cryptomator',\n",
              "  'https://github.com/LumingSun/ML4DB-paper-list',\n",
              "  'https://github.com/halfrost/Halfrost-Field',\n",
              "  'https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils',\n",
              "  'https://github.com/cockroachdb/cockroach',\n",
              "  'https://github.com/fnproject/fn',\n",
              "  'https://github.com/Tencent/VasSonic'],\n",
              " [0.10713447896343649,\n",
              "  0.06813319305446196,\n",
              "  0.06122971289014981,\n",
              "  0.057355690441885765,\n",
              "  0.05331318891452002,\n",
              "  0.05201791861750958,\n",
              "  0.048158459087971194,\n",
              "  0.047936336649312856,\n",
              "  0.046609899910211416,\n",
              "  0.04508615318328211])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.process_query('Kubernetes OR docker AND tutorial')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I03PBdLsJvri",
        "outputId": "4cb284bc-44e0-49cf-b31d-04552e030ae9"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['https://github.com/BretFisher/udemy-docker-mastery',\n",
              "  'https://github.com/kelseyhightower/kubernetes-the-hard-way',\n",
              "  'https://github.com/googleforgames/agones',\n",
              "  'https://github.com/apache/openwhisk',\n",
              "  'https://github.com/ToolJet/ToolJet',\n",
              "  'https://github.com/armosec/kubescape',\n",
              "  'https://github.com/dgkanatsios/CKAD-exercises',\n",
              "  'https://github.com/docker/compose',\n",
              "  'https://github.com/falcosecurity/falco',\n",
              "  'https://github.com/dotnet-architecture/eShopOnContainers'],\n",
              " [0.26197910474737485,\n",
              "  0.2389234022954762,\n",
              "  0.2265280444282356,\n",
              "  0.19445904048361248,\n",
              "  0.16016655444814767,\n",
              "  0.14618747608648072,\n",
              "  0.14523016885373158,\n",
              "  0.14011337790951978,\n",
              "  0.13744752510717267,\n",
              "  0.13192309827125104])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.process_query('accelerate deep learning applications AND GPU OR CPU')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ogqqIsKZ0R",
        "outputId": "4acf0919-8c02-44aa-bb00-8dcfc1255e29"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['https://github.com/amusi/Deep-Learning-Interview-Book',\n",
              "  'https://github.com/NVIDIA/DALI',\n",
              "  'https://github.com/thibo73800/aihub',\n",
              "  'https://github.com/NervanaSystems/neon',\n",
              "  'https://github.com/tflearn/tflearn',\n",
              "  'https://github.com/symisc/sod',\n",
              "  'https://github.com/microsoft/computervision-recipes',\n",
              "  'https://github.com/pierpaolo28/Artificial-Intelligence-Projects',\n",
              "  'https://github.com/eclipse/deeplearning4j',\n",
              "  'https://github.com/floydhub/dl-docker'],\n",
              " [0.17176341255023447,\n",
              "  0.17116339327684793,\n",
              "  0.1608545304953879,\n",
              "  0.13656153990823058,\n",
              "  0.1307579381027511,\n",
              "  0.1281080389708672,\n",
              "  0.12735904654914149,\n",
              "  0.12479473138052749,\n",
              "  0.12072464613463144,\n",
              "  0.11951162854857461])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.process_query('React web application development NOT webStorm')\n",
        "result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbEIFv7TTrW5",
        "outputId": "c5ac2a15-a0a6-4e98-e758-114f38bf794c"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['https://github.com/react-native-elements/react-native-elements',\n",
              "  'https://github.com/facebook/react-native',\n",
              "  'https://github.com/invertase/react-native-firebase',\n",
              "  'https://github.com/microsoft/fluentui',\n",
              "  'https://github.com/alan2207/bulletproof-react',\n",
              "  'https://github.com/reactide/reactide',\n",
              "  'https://github.com/GeekyAnts/vue-native-core',\n",
              "  'https://github.com/feathersjs/feathers',\n",
              "  'https://github.com/midwayjs/midway',\n",
              "  'https://github.com/roughike/inKino'],\n",
              " [0.15496579495813756,\n",
              "  0.14177082182649722,\n",
              "  0.13259730273182554,\n",
              "  0.1310721414397063,\n",
              "  0.12409283716008174,\n",
              "  0.12186330485250957,\n",
              "  0.12117623524967125,\n",
              "  0.12050926755228072,\n",
              "  0.11442715371979514,\n",
              "  0.11341564505209982])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBjL2JtfdMcV"
      },
      "source": [
        "<div dir = \"rtl\">\n",
        "<font size=3>\n",
        "<b>Shunt Algorithm for Boolean Retrival:</b> \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvFoUHnbdKeX"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "الگوریتم Shunt یک الگوریتم برای تبدیل یک عبارت infix به postfix است. درصورتی که کوئری وارد شده دارای اولویت و پرانتزگذاری باشد، ابتدا این الگوریتم را روی کوئری صدا میزنیم. پس از آنکه کوئری به حالت postfix در بیاید میتوان با اجرای کوئری و استفاده از استک برای نگه داری بخش های مختلف کوئری،‌ جواب را پیدا کرد. \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = \"rtl\">\n",
        "<font size=3>\n",
        "<b>Tf-idf:</b> \n",
        "</div>"
      ],
      "metadata": {
        "id": "X1zAn98EPPgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "از کلاس TfidfVectorizer در ماژول sklearn.feature_extraction.text استفاده می‌کنیم. 'input='content را استفاده می‌کنیم که ورودی را string در نظر بگیرد، 'analyzer='word استفاده می‌کنیم تا پردازش در سطح کلمه باشد. از مدل unigram نیز استفاده می‌کنیم. کلماتی که در بیشتر از 0.9 داکیومنت‌ها هستند را نادیده می‌گیریم. از 'L2 norm' برای normalize هر سطر استفاده می‌کنیم. همچنین به جای tf از 1 + log(tf) برای به جای tf در رابطه tf-idf استفاده می‌کنیم. از smoothing نیز استفاده می کنیم به طوری که یک را با فرکانس داکیومنت‌ها جمع می‌کنیم تا از تقسیم شدن به صفر جلوگیری کند. MRR در این بخش برابر با 0.82 است.\n",
        "</div>"
      ],
      "metadata": {
        "id": "dyR7aAtdOpAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Tfidf()\n",
        "model.print_results(query='applications of deep learning')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg40SdJbLxfp",
        "outputId": "2afa26af-a367-4146-8329-af096a24c4e7"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url: https://github.com/amusi/Deep-Learning-Interview-Book, score: 0.33808\n",
            "url: https://github.com/thibo73800/aihub, score: 0.31660\n",
            "url: https://github.com/ankonzoid/artificio, score: 0.23456\n",
            "url: https://github.com/tirthajyoti/Papers-Literature-ML-DL-RL-AI, score: 0.20274\n",
            "url: https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code, score: 0.19498\n",
            "url: https://github.com/rasmusbergpalm/DeepLearnToolbox, score: 0.19268\n",
            "url: https://github.com/openai/spinningup, score: 0.18900\n",
            "url: https://github.com/fchollet/deep-learning-with-python-notebooks, score: 0.18619\n",
            "url: https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook, score: 0.18137\n",
            "url: https://github.com/khanhnamle1994/computer-vision, score: 0.18107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Tfidf()\n",
        "model.print_results(query='Game studio projects')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FARtOlEUMHC8",
        "outputId": "af3f7b1b-5ea9-4696-b5ab-335ce0115139"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url: https://github.com/chrismaltby/gb-studio, score: 0.24864\n",
            "url: https://github.com/cping/LGame, score: 0.14312\n",
            "url: https://github.com/watabou/pixel-dungeon, score: 0.13861\n",
            "url: https://github.com/stride3d/stride, score: 0.13423\n",
            "url: https://github.com/FlaxEngine/FlaxEngine, score: 0.13415\n",
            "url: https://github.com/cocos2d/cocos2d-x, score: 0.13239\n",
            "url: https://github.com/WillKoehrsen/ai-projects, score: 0.12867\n",
            "url: https://github.com/CKGrafico/Cordova-Multiplatform-Template, score: 0.12794\n",
            "url: https://github.com/ppy/osu, score: 0.12577\n",
            "url: https://github.com/UnityTechnologies/open-project-1, score: 0.11988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Tfidf()\n",
        "model.print_results(query='visual recognition and image segmentation tasks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFZsRCG6MWnx",
        "outputId": "b417017b-da86-4dcd-c29f-880af770dd5d"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url: https://github.com/khanhnamle1994/computer-vision, score: 0.20714\n",
            "url: https://github.com/yassouali/ML-paper-notes, score: 0.18564\n",
            "url: https://github.com/hoya012/CVPR-2019-Paper-Statistics, score: 0.17400\n",
            "url: https://github.com/amusi/awesome-ai-awesomeness, score: 0.15092\n",
            "url: https://github.com/ArcherFMY/Paper_Reading_List, score: 0.14821\n",
            "url: https://github.com/bnosac/image, score: 0.14183\n",
            "url: https://github.com/google-research/scenic, score: 0.13965\n",
            "url: https://github.com/balavenkatesh3322/CV-pretrained-model, score: 0.13816\n",
            "url: https://github.com/SnailTyan/deep-learning-papers-translation, score: 0.13588\n",
            "url: https://github.com/microsoft/computervision-recipes, score: 0.13500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = \"rtl\">\n",
        "<font size=3>\n",
        "<b>FastText:</b> \n",
        "</div>"
      ],
      "metadata": {
        "id": "YRf6BI9YgSCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = \"rtl\">\n",
        "در این روش، در ابتدا مدل را با استفاده از توکن‌های مربوط به README ها ترین میکنیم. همچنین امبدینگ مربوط به هر داک را محاسبه میکنیم. پس از آن کوئری را دریافت کرده و با همان مراحلی که داک‌ها را توکنایز کرده بودیم،‌این کوئری را هم توکنایز و نرمالایز می کنیم. هدف آن است که متن و کوئری در یک فضای کلمات قرار گیرند. در گام بعدی بردار امبدینگ مربوط به کوئری را هم حساب میکنیم. در انتها برای آنکه مشخص شود فاصله بین کدام دو بردار امبدینگ (کوئری و داک ها) از همه کمتر است،‌ فاصله کسینوسی را حساب میکنیم. هرچقدر این مقدار کمتر باشد، دو بردار نزدیکتر بوده و در نتیجه داک در رتبه بالاتری است. البته قابل توجه است که مدل های آماده pretrained برا ترین کردن اولیه بهتر از ترین کردن با دیتا به صورت unsupervised است. اما چون کوئری نیز از همان فضای کلمات متن ها آمده است، در نتیجه مشکل زیادی پیش نمی آید. \n",
        "مقدار mrr عدد 0.81 است.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "5QIlEDp5p9an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = ft_model.search_query('deep learning tutorial')\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ4P-pPbA8-O",
        "outputId": "ff5c4424-a6e3-4d7f-fc15-773255fe86fc"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://github.com/amusi/Deep-Learning-Interview-Book', 0.9077829122543335),\n",
              " ('https://github.com/lexfridman/mit-deep-learning', 0.8877608776092529),\n",
              " ('https://github.com/nivu/ai_all_resources', 0.8830912709236145),\n",
              " ('https://github.com/goodrahstar/my-awesome-AI-bookmarks',\n",
              "  0.8822774291038513),\n",
              " ('https://github.com/mbadry1/DeepLearning.ai-Summary', 0.879920244216919),\n",
              " ('https://github.com/thibo73800/aihub', 0.8784650564193726),\n",
              " ('https://github.com/tirthajyoti/Papers-Literature-ML-DL-RL-AI',\n",
              "  0.877159833908081),\n",
              " ('https://github.com/khanhnamle1994/computer-vision', 0.8743525743484497),\n",
              " ('https://github.com/fchollet/deep-learning-with-python-notebooks',\n",
              "  0.8736817836761475),\n",
              " ('https://github.com/rasmusbergpalm/DeepLearnToolbox', 0.8701558113098145)]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = ft_model.search_query('deploy and maintanance')\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jum1MwXOBGVl",
        "outputId": "3cb7ce88-4213-4eef-8f6c-8cbabb6e61d5"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://github.com/Netflix/chaosmonkey', 0.8745149374008179),\n",
              " ('https://github.com/spinnaker/spinnaker', 0.8625348806381226),\n",
              " ('https://github.com/hashicorp/nomad', 0.8613294959068298),\n",
              " ('https://github.com/vercel/vercel', 0.852674126625061),\n",
              " ('https://github.com/anaibol/awesome-serverless', 0.8515518307685852),\n",
              " ('https://github.com/ToolJet/ToolJet', 0.8509125709533691),\n",
              " ('https://github.com/BretFisher/udemy-docker-mastery', 0.8507421612739563),\n",
              " ('https://github.com/traefik/traefik', 0.8492861390113831),\n",
              " ('https://github.com/aws/aws-cdk', 0.8492258787155151),\n",
              " ('https://github.com/donnemartin/awesome-aws', 0.8476577401161194)]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = ft_model.search_query('ios with swift')\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDkim0JzA4x2",
        "outputId": "03592297-054e-44da-b4be-a39fb1a8eb56"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://github.com/Aufree/trip-to-iOS', 0.8616037368774414),\n",
              " ('https://github.com/jordansinger/messages-multiplatform-swiftui-sample',\n",
              "  0.8342176079750061),\n",
              " ('https://github.com/dkhamsing/open-source-ios-apps', 0.7441614866256714),\n",
              " ('https://github.com/draveness/analyze', 0.7218459248542786),\n",
              " ('https://github.com/ibireme/YYKit', 0.7206997871398926),\n",
              " ('https://github.com/didi/DoKit', 0.6901710629463196),\n",
              " ('https://github.com/Tim9Liu9/TimLiu-iOS', 0.6850401163101196),\n",
              " ('https://github.com/vsouza/awesome-ios', 0.6763685345649719),\n",
              " ('https://github.com/SnapKit/SnapKit', 0.6589732766151428),\n",
              " ('https://github.com/h2y/Shadowrocket-ADBlock-Rules', 0.6556643843650818)]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = \"rtl\">\n",
        "<font size=3>\n",
        "<b>Transformer:</b> \n",
        "</div>"
      ],
      "metadata": {
        "id": "LgZIVihZgZPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = \"rtl\">\n",
        "​​ما از مدل distilbert-base-nli-stsb-mean-tokens استفاده خواهیم کرد که بهترین عملکرد را در تسک های Textual Similarity  در بین نسخه های DistilBERT دارد. علاوه بر این اگرچه این مدل کمی بدتر از BERT است، اما به دلیل داشتن اندازه کوچکتر، بسیار سریعتر است.\n",
        " \n",
        "Faiss  کتابخانه ای برای جستجوی شباهت و خوشه بندی بردارهای دنس است. این شامل الگوریتم‌هایی است که در مجموعه‌هایی از بردارها با هر اندازه حتی آنهایی که در RAM جا نمی‌شوند جستجو می ‌کنند.\n",
        " \n",
        "این یک index ساده که یک جست و جو با فاصله‌ی L2 brute-force که به صورت خطی افزایش پیدا میکند انجام میدهد.\n",
        "برای ایجاد یک index با بردارهای بدنه readme  این کار را انجام می دهیم:\n",
        "نوع داده بردارهای readme را به float32 تغییر میدهیم.یک index میسازیم و به اندازه بعد بردارهایی که روی آنها عملیات انجام خواهد شد.ایندکس را به IndexIDMap ارسال میکنیم و آبجکتی که  شیئی که به ما امکان می‌دهد یک لیست دلخواه از شناسه‌ها برای بردارهای ایندکس شده ارائه کنیم.بردارهای readme و نقشه شناسه آنها را به ایندکس اضافه میکنیم که بردارها را به شناسه readme آنها مپ میکنیم.\n",
        " \n",
        "ایندکسی که ما ساختیم یک جستجوی KNN را انجام می دهد و در آخر باید تعداد همسایه هایی که باید برگردانده شوند را نشان میدهیم.\n",
        " مقدار mrr برابر با 0.86 است. \n",
        "</div>\n",
        "\n",
        "q1 = \"deep learning papers\"\n",
        "\n",
        "```\n",
        "L2 distance: [205.97442626953125, 210.46890258789062, 213.1353302001953, 213.1353302001953, 214.97361755371094, 218.3271484375, 220.17562866210938, 223.4440460205078, 223.4635772705078, 223.55410766601562]\n",
        "\n",
        "Readme IDs: [467, 74, 23, 464, 1377, 1082, 270, 508, 172, 1285]\n",
        "['https://github.com/tflearn/tflearn',\n",
        " 'https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap',\n",
        " 'https://github.com/kmario23/deep-learning-drizzle',\n",
        " 'https://github.com/kmario23/deep-learning-drizzle',\n",
        " 'https://github.com/coqui-ai/TTS',\n",
        " 'https://github.com/Kulbear/deep-learning-coursera',\n",
        " 'https://github.com/d2l-ai/d2l-en',\n",
        " 'https://github.com/baidu-research/DeepBench',\n",
        " 'https://github.com/ChristosChristofidis/awesome-deep-learning',\n",
        " 'https://github.com/david-gpu/srez']\n",
        "```\n",
        "q2 = \"learning resources for computer vision\"\n",
        "\n",
        "```\n",
        "L2 distance: [138.74569702148438, 169.27291870117188, 173.50767517089844, 176.0240936279297, 178.26512145996094, 179.61874389648438, 182.32862854003906, 183.37486267089844, 185.71066284179688, 185.7895965576172]\n",
        "\n",
        "Readme IDs: [682, 1330, 420, 1138, 132, 1281, 415, 1232, 1187, 318]\n",
        "['https://github.com/kenjihata/cs231a-notes',\n",
        " 'https://github.com/ankonzoid/artificio',\n",
        " 'https://github.com/llSourcell/Learn_Computer_Vision',\n",
        " 'https://github.com/khanhnamle1994/computer-vision',\n",
        " 'https://github.com/microsoft/muzic',\n",
        " 'https://github.com/dformoso/machine-learning-mindmap',\n",
        " 'https://github.com/aws-samples/aws-machine-learning-university-accelerated-cv',\n",
        " 'https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV',\n",
        " 'https://github.com/dusty-nv/jetson-inference',\n",
        " 'https://github.com/chainer/chainercv']\n",
        " ```\n",
        "q3 = \"infrastructure based on the asp.net core\"\n",
        "\n",
        "```\n",
        "L2 distance: [202.02178955078125, 202.56175231933594, 203.72470092773438, 205.60671997070312, 212.73658752441406, 213.1620635986328, 216.88650512695312, 219.00022888183594, 220.93382263183594, 221.25343322753906]\n",
        "\n",
        "Readme IDs: [475, 818, 772, 526, 251, 711, 1347, 1245, 88, 873]\n",
        "['https://github.com/thingsboard/thingsboard',\n",
        " 'https://github.com/bridgecrewio/checkov',\n",
        " 'https://github.com/abpframework/abp',\n",
        " 'https://github.com/aspnetboilerplate/aspnetboilerplate',\n",
        " 'https://github.com/ReactiveX/RxSwift',\n",
        " 'https://github.com/alibaba/x-deeplearning',\n",
        " 'https://github.com/graphql/dataloader',\n",
        " 'https://github.com/Jackpopc/aiLearnNotes',\n",
        " 'https://github.com/hashicorp/terraform',\n",
        " 'https://github.com/dotnet-architecture/eShopOnWeb']\n",
        "```\n",
        "q4 = \"nlp chat bots\"\n",
        "\n",
        "```\n",
        "L2 distance: [197.46856689453125, 198.1324005126953, 202.7813720703125, 208.62408447265625, 208.9859619140625, 211.4918670654297, 212.06109619140625, 213.52980041503906, 222.63330078125, 222.7926483154297]\n",
        "\n",
        "Readme IDs: [946, 826, 608, 432, 860, 950, 1185, 1047, 145, 1382]\n",
        "['https://github.com/BotLibre/BotLibre',\n",
        " 'https://github.com/LeiWang1999/ZYNQ-NVDLA',\n",
        " 'https://github.com/DevDungeon/Cathy',\n",
        " 'https://github.com/chrisalbon/mlai',\n",
        " 'https://github.com/RubensZimbres/Repo-2016',\n",
        " 'https://github.com/yanshengjia/artificial-intelligence',\n",
        " 'https://github.com/deepmipt/DeepPavlov',\n",
        " 'https://github.com/andelf/PyAIML',\n",
        " 'https://github.com/getsentry/sentry',\n",
        " 'https://github.com/aidlearning/AidLearning-FrameWork']\n",
        " ```\n",
        "q7 = \"password manager\"\n",
        "\n",
        " ```\n",
        "L2 distance: [155.17620849609375, 194.96591186523438, 251.89572143554688, 255.93499755859375, 262.82598876953125, 263.5931701660156, 264.57269287109375, 267.79254150390625, 268.3603210449219, 271.2806701660156]\n",
        "\n",
        "Readme IDs: [355, 1153, 1249, 1385, 570, 1191, 840, 402, 577, 565]\n",
        "['https://github.com/marcwebbie/passpie',\n",
        " 'https://github.com/keepassxreboot/keepassxc',\n",
        " 'https://github.com/keycloak/keycloak',\n",
        " 'https://github.com/houtianze/bypy',\n",
        " 'https://github.com/jpillora/cloud-torrent',\n",
        " 'https://github.com/owncloud/core',\n",
        " 'https://github.com/kzlekk/HWSensors',\n",
        " 'https://github.com/Gazler/githug',\n",
        " 'https://github.com/facebookarchive/RakNet',\n",
        " 'https://github.com/GoogleCloudPlatform/python-docs-samples']\n",
        " ```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vXya-2tGgdIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir = \"rtl\">\n",
        "<font size=3>\n",
        "<b>گسترش پرسمان</b> \n",
        "\n",
        "در این بخش، برای چهار متدی که قبلا پیاده سازی شده بود، گسترش کوئری انجام شده است. \n",
        "برای ولین متفاوت از سایر بخش ها است. در بازیابی بولین تعدادی کلمه داریم که با AND\n",
        "یا\n",
        "OR\n",
        "یا \n",
        "NOT\n",
        "از هم جدا شده اند. برای هریک از کلمات، باساتفاده از مدل \n",
        "Fasttext\n",
        "که از قبل داشتیم، کلمات مشابه را پیدا میکنیم. سپس هر کوئری را تبدیل به چندین کوئری میکنیم به صورتی که به طور رندوم، کلمات را با معادل های آن ها در کوئری جایگزین میکنیم. \n",
        "\n",
        "سپس تمام کوئری ها را بازیابی کرده و خروجی نهایی حاصل احتماع آن ها است. \n",
        "\n",
        "برای سه مدل دیگر، یک نوع rocchio\n",
        "پیاده سازی شده است. به این صورت که بردار کوئری با استفاده از بردار داکهای مرتبط و غیرمرتبط آپدیت میشود. در بخش زیر کد مربوط به این بخش پیاده سازی شده است. توجه داریم که برای تمام سه الگوریتم تنها یک پیاده سازی کفایت دارد و در هریک بسته به نوع امبدینگ، ورودی ها متفاوت خواهند بود. باتوجه به اکسل مربوط به MRR\n",
        "بعد از بهبود کوئری، خروجی ها به شکل معناداری مرتبط تر شده اند. \n",
        "</div>"
      ],
      "metadata": {
        "id": "AvJwuf27fy1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rocchio Algorithm "
      ],
      "metadata": {
        "id": "kYunkry3CsJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "class Rocchio:\n",
        "\n",
        "    def __init__(self, alpha = 1, beta = 0.75, gamma = 0.15, iters = 2, rel_count = 7, nrel_count = 3):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.iters = iters\n",
        "        self.rel_count = rel_count\n",
        "        self.nrel_count = nrel_count\n",
        "\n",
        "    def expand_query(self, sorted_docs, query, urls = False):\n",
        "        '''\n",
        "        sorted_docs are the result of retrival\n",
        "        query is a vectorized\n",
        "        output is a numpy array \n",
        "        '''\n",
        "        \n",
        "        sorted_docs = np.array(sorted_docs)\n",
        "        query = np.array(query)\n",
        "\n",
        "        cos_sim = cosine_similarity(np.array([query]), sorted_docs)\n",
        "        rankings = np.flip(cos_sim.argsort(axis=1), axis=1)\n",
        "        for _ in range(self.iters):\n",
        "            rel_vecs = sorted_docs[rankings[:, :self.rel_count]].mean(axis=1)\n",
        "            nrel_vecs = sorted_docs[rankings[:, -self.nrel_count:]].mean(axis=1)\n",
        "            query = self.alpha * query + self.beta * rel_vecs - self.gamma * nrel_vecs\n",
        "            cos_sim = cosine_similarity(query, sorted_docs)\n",
        "            rankings = np.flip(cos_sim.argsort(axis=1), axis=1)\n",
        "        if type(urls) ==bool:\n",
        "            return sorted_docs[rankings[:, :]], cos_sim[rankings[:,:]]\n",
        "        else:\n",
        "            urls = np.array(urls)\n",
        "            return sorted_docs[rankings[:, :]], urls[rankings[:,:]], cos_sim[0][rankings[:,:]]\n",
        "\n",
        "            \n"
      ],
      "metadata": {
        "id": "BX46GmIsCvs0"
      },
      "execution_count": 64,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FY00D2DSCuzO",
        "LFwZu2djRf8o",
        "0XH-iAg3MyLk",
        "v3ujQ5pOBozu"
      ],
      "name": "HW3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}